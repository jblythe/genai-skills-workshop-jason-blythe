{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "u6ZGqBfvqZz7kWneiifLnUOm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6ZGqBfvqZz7kWneiifLnUOm",
        "outputId": "3e3b9032-6274-4b96-bccc-c4d99d54ebf4",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --user --quiet google-cloud-aiplatform google-cloud-bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0jaq-qwmn9lh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jaq-qwmn9lh",
        "outputId": "18e0e203-335a-463f-e5a3-bc68b85847a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwiklabs-gcp-04-ee8165cd97c8\n"
          ]
        }
      ],
      "source": [
        "# Get project ID\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "DATASET_LOCATION = \"US\"\n",
        "print(PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "wpCJi6HQoELC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpCJi6HQoELC",
        "outputId": "8957e8e4-dfe3-4387-980a-e728697a249c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "print(\"Initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "01e682b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01e682b2",
        "outputId": "f905ac73-6a64-4b88-94c4-5ec7b107d04d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset `qwiklabs-gcp-04-ee8165cd97c8.genai_skills_workshop` in US.\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound\n",
        "\n",
        "# BigQuery dataset/table configuration.\n",
        "DATASET_ID_REQUESTED = \"genai-skills-workshop\"\n",
        "# BigQuery converts hyphens to underscores in dataset IDs.\n",
        "DATASET_ID = DATASET_ID_REQUESTED.replace(\"-\", \"_\")\n",
        "TABLE_ID = \"aurora_bay_faqs\"\n",
        "GCS_URI = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
        "\n",
        "# Reuse the same project inferred earlier in the notebook.\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Build dataset reference anchored to the desired location.\n",
        "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
        "dataset_ref.location = DATASET_LOCATION\n",
        "\n",
        "# Create the dataset if it does not yet exist.\n",
        "try:\n",
        "  bq_client.get_dataset(dataset_ref)\n",
        "  print(f\"Dataset `{PROJECT_ID}.{DATASET_ID}` already exists.\")\n",
        "except NotFound:\n",
        "  bq_client.create_dataset(dataset_ref)\n",
        "  print(f\"Created dataset `{PROJECT_ID}.{DATASET_ID}` in {DATASET_LOCATION}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "9311aee6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9311aee6",
        "outputId": "79bdc60b-57e8-4eca-dda0-0ca4b56d545e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting load job 689c228b-9a8f-4cdc-a8e7-ce5c3a607078\n",
            "Loaded 50 rows into qwiklabs-gcp-04-ee8165cd97c8:genai_skills_workshop.aurora_bay_faqs.\n"
          ]
        }
      ],
      "source": [
        "# Fully qualified table name we will load the CSV into.\n",
        "table_id = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "# Configure the ingestion job to use CSV autodetect and to overwrite any prior data.\n",
        "load_job_config = bigquery.LoadJobConfig(\n",
        "  source_format=bigquery.SourceFormat.CSV,\n",
        "  skip_leading_rows=1,\n",
        "  autodetect=True,\n",
        "  write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        ")\n",
        "\n",
        "# Launch the load job from the public GCS bucket into BigQuery.\n",
        "load_job = bq_client.load_table_from_uri(GCS_URI, table_id, job_config=load_job_config)\n",
        "print(f\"Starting load job {load_job.job_id}\")\n",
        "\n",
        "# Wait for the load job to complete and fetch metadata for confirmation.\n",
        "load_job.result()\n",
        "table = bq_client.get_table(table_id)\n",
        "print(f\"Loaded {table.num_rows} rows into {table.full_table_id}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1a63d9d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a63d9d3",
        "outputId": "b6912600-f5c2-4839-ac7e-5f458e3184a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created connection `projects/376838992535/locations/us/connections/vertex_ai_text_embeddings` for Vertex AI embeddings.\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import bigquery_connection_v1\n",
        "\n",
        "# Connection configuration that allows BigQuery to call Vertex AI embeddings.\n",
        "CONNECTION_LOCATION = \"us\"\n",
        "CONNECTION_ID = \"vertex_ai_text_embeddings\"\n",
        "connection_parent = f\"projects/{PROJECT_ID}/locations/{CONNECTION_LOCATION}\"\n",
        "connection_name = f\"{connection_parent}/connections/{CONNECTION_ID}\"\n",
        "\n",
        "connection_client = bigquery_connection_v1.ConnectionServiceClient()\n",
        "\n",
        "# Reuse the connection when it already exists, otherwise create a new Vertex AI link.\n",
        "try:\n",
        "  connection = connection_client.get_connection(name=connection_name)\n",
        "  print(f\"Connection `{connection.name}` already exists.\")\n",
        "except NotFound:\n",
        "  conn = bigquery_connection_v1.types.Connection(\n",
        "      cloud_resource=bigquery_connection_v1.types.CloudResourceProperties()\n",
        "  )\n",
        "  connection = connection_client.create_connection(\n",
        "      parent=connection_parent,\n",
        "      connection_id=CONNECTION_ID,\n",
        "      connection=conn,\n",
        "  )\n",
        "  print(f\"Created connection `{connection.name}` for Vertex AI embeddings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5c07c034",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c07c034",
        "outputId": "334f7aa9-04bf-42d0-d3b2-6ed563c30a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created or updated remote model `qwiklabs-gcp-04-ee8165cd97c8.genai_skills_workshop.embedding_model` targeting text-embedding-005.\n"
          ]
        }
      ],
      "source": [
        "# Define a remote BigQuery ML model that proxies requests to the Vertex AI text-embedding-005 endpoint.\n",
        "remote_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATASET_ID}.embedding_model`\n",
        "REMOTE WITH CONNECTION `{DATASET_LOCATION}.{CONNECTION_ID}`\n",
        "OPTIONS (\n",
        "  endpoint = 'text-embedding-005'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Execute the DDL so the model can be used in subsequent ML.GENERATE_TEXT_EMBEDDING calls.\n",
        "query_job = bq_client.query(remote_model_sql)\n",
        "query_job.result()\n",
        "print(f\"Created or updated remote model `{PROJECT_ID}.{DATASET_ID}.embedding_model` targeting text-embedding-005.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c34f11f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34f11f6",
        "outputId": "9411f83c-7cc0-4faa-b132-d0f121cdab3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema for aurora_bay_faqs:\n",
            "  string_field_0: STRING\n",
            "  string_field_1: STRING\n"
          ]
        }
      ],
      "source": [
        "# Inspect the newly loaded table to understand available fields for downstream processing.\n",
        "table = bq_client.get_table(table_id)\n",
        "print(\"Schema for aurora_bay_faqs:\")\n",
        "for field in table.schema:\n",
        "  print(f\"  {field.name}: {field.field_type}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "e2082fa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2082fa5",
        "outputId": "bb0cc99e-18b9-466c-deca-5c6e86214e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created table `qwiklabs-gcp-04-ee8165cd97c8.genai_skills_workshop.aurora_bay_faqs_with_embeddings` with concatenated QA embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Materialize a table that stores concatenated Q/A text alongside its embedding vector.\n",
        "EMBEDDING_TABLE_ID = \"aurora_bay_faqs_with_embeddings\"\n",
        "embedding_table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{EMBEDDING_TABLE_ID}\"\n",
        "\n",
        "embedding_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{embedding_table_ref}` AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.GENERATE_TEXT_EMBEDDING(\n",
        "    MODEL `{DATASET_ID}.embedding_model`,\n",
        "    (SELECT string_field_0 as question, string_field_1 as answer, CONCAT(string_field_0, ': ', string_field_0) AS content FROM {PROJECT_ID}.{DATASET_ID}.{TABLE_ID})\n",
        "  );\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Run the transformation query and wait for completion.\n",
        "embedding_job = bq_client.query(embedding_sql)\n",
        "embedding_job.result()\n",
        "print(f\"Created table `{embedding_table_ref}` with concatenated QA embeddings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "u65oV6zPoGSy",
      "metadata": {
        "id": "u65oV6zPoGSy"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "def answer_question_gemini(prompt):\n",
        "  \"\"\"Invoke Gemini with consistent generation settings to answer a prompt.\"\"\"\n",
        "  model = GenerativeModel(\"gemini-2.5-flash-lite\")\n",
        "  response = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.5,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "  stream=False,\n",
        "  )\n",
        "  try:\n",
        "    return response.text\n",
        "  except:\n",
        "    print(\"An Error Ocuured Cleaning the Data\")\n",
        "    return \"An Error Ocuured Cleaning the Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "YCgf9L4toQ3U",
      "metadata": {
        "id": "YCgf9L4toQ3U"
      },
      "outputs": [],
      "source": [
        "def run_search(question):\n",
        "  from google.cloud import bigquery\n",
        "\n",
        "  client = bigquery.Client()\n",
        "\n",
        "  # Perform a vector search over the FAQ embedding table, using the remote model\n",
        "  # to embed the incoming natural language question on the fly.\n",
        "  sql = f\"\"\"\n",
        "      SELECT base.question, base.answer\n",
        "      FROM VECTOR_SEARCH(\n",
        "      TABLE `{embedding_table_ref}`, 'text_embedding',\n",
        "      (\n",
        "      SELECT text_embedding, content AS query\n",
        "      FROM ML.GENERATE_TEXT_EMBEDDING(MODEL `{PROJECT_ID}.{DATASET_ID}.embedding_model`,\n",
        "          (SELECT @question AS content))),\n",
        "      top_k => 5)\n",
        "      \"\"\"\n",
        "\n",
        "  # Bind the user-entered question as a parameter to avoid SQL injection and reuse cached plans.\n",
        "  job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters=[\n",
        "        bigquery.ScalarQueryParameter(\"question\", \"STRING\", question),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  query_job = client.query(sql, job_config=job_config)\n",
        "\n",
        "  # Format retrieved Q/A pairs as plain text paragraphs for downstream prompting.\n",
        "  rows = []\n",
        "  for row in query_job:\n",
        "    rows.append(f\"Q: {row.question}\\nA: {row.answer}\")\n",
        "\n",
        "  return \"\\n\\n\".join(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "h6chC55poWof",
      "metadata": {
        "id": "h6chC55poWof"
      },
      "outputs": [],
      "source": [
        "def build_prompt(data, question):\n",
        "  \"\"\"Wrap retrieved context in a simple instruction-following prompt.\"\"\"\n",
        "  prompt = \"\"\"\n",
        "    Instructions: Answer the question using the following Context.\n",
        "\n",
        "    Context: {0}\n",
        "\n",
        "    Question: {1}\n",
        "  \"\"\".format(data, question)\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "kpiT01ZXoZok",
      "metadata": {
        "id": "kpiT01ZXoZok"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def answer_question(question):\n",
        "  \"\"\"Retrieve FAQ context, expose it inline, and ask Gemini for a final answer.\"\"\"\n",
        "\n",
        "  data = run_search(question)\n",
        "  display(\"Retrieved Data:\")\n",
        "  display(data)\n",
        "  display(\" . . . \")\n",
        "  prompt = build_prompt(data, question)\n",
        "  answer_gemini = answer_question_gemini(prompt)\n",
        "\n",
        "  return answer_gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "HHAjjSX-odDk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "HHAjjSX-odDk",
        "outputId": "c7ad49db-c00c-49ea-dfb6-29bea3e2f674"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Retrieved Data:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Q: What types of recreation are available in Aurora Bay?\\nA: Popular activities include fishing, kayaking, hiking in the nearby forests, and northern lights viewing in the winter.\\n\\nQ: What cultural events are unique to Aurora Bay?\\nA: Besides the Aurora Lights Winter Festival, there is the Seafarers’ Gathering in August, celebrating local fishing traditions with boat parades and seafood cook-offs.\\n\\nQ: What local outdoor adventure companies operate in Aurora Bay?\\nA: North Star Excursions and Bay Explorers offer guided hikes, kayak tours, and wilderness camping experiences for visitors and residents.\\n\\nQ: What is the Aurora Lights Winter Festival?\\nA: It’s Aurora Bay’s annual winter celebration featuring ice carving competitions, northern lights viewing tours, live music, and local food vendors. It typically takes place in late January.\\n\\nQ: Are there any hotels or lodging options in Aurora Bay?\\nA: Yes. Options include the Aurora Bay Lodge (near the harbor), several bed-and-breakfasts, and a small hostel on Main Street.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "' . . . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'User Question:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'What are some fun activities in aurora?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'--------------------------------'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Gemini Answer:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Based on the context provided, some fun activities in Aurora Bay include:\\n\\n*   Fishing\\n*   Kayaking\\n*   Hiking in the nearby forests\\n*   Northern lights viewing (especially in winter)\\n*   Attending the Aurora Lights Winter Festival (which features ice carving, live music, and local food)\\n*   Participating in the Seafarers’ Gathering (with boat parades and seafood cook-offs)\\n*   Guided hikes, kayak tours, and wilderness camping experiences offered by local companies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "QUESTION = \"What are some fun activities in aurora?\"\n",
        "\n",
        "answer_gemini = answer_question(QUESTION)\n",
        "display(\"User Question:\")\n",
        "display(QUESTION)\n",
        "display(\"--------------------------------\")\n",
        "display(\"Gemini Answer:\")\n",
        "display(answer_gemini)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab_2_bigquery_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}